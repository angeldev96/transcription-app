{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Install Packages\n!pip install -q -U nemo_toolkit['asr'] pydub pandas fastapi uvicorn python-multipart pyngrok torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118 wget\n!pip install numpy==1.26.4 --force-reinstall\n# Add moviepy for video processing\n!pip install moviepy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-15T00:00:59.551637Z","iopub.execute_input":"2025-05-15T00:00:59.551836Z","iopub.status.idle":"2025-05-15T00:04:48.519542Z","shell.execute_reply.started":"2025-05-15T00:00:59.551818Z","shell.execute_reply":"2025-05-15T00:04:48.518501Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.4/79.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m955.6/955.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m845.4/845.4 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.8/419.8 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.3/94.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for sox (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for texterrors (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for kaldi-python-io (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\ncuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\ncudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\ndistributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 4.24.4 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow-metadata 1.17.0 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 4.24.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\ngoogle-cloud-bigtable 2.30.0 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0+cu118 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting numpy==1.26.4\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\ndask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\ncuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\ncudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\ndistributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0+cu118 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.4\nRequirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (1.0.3)\nRequirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.4.2)\nRequirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.67.1)\nRequirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.32.3)\nRequirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.1.11)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from moviepy) (1.26.4)\nRequirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.37.0)\nRequirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.6.0)\nRequirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.1.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2025.4.26)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 2: Imports and Global Model Loading\nimport nemo.collections.asr as nemo_asr\nimport torch\nfrom pathlib import Path\nfrom pydub import AudioSegment\nimport os\nimport shutil\nimport uvicorn\nimport nest_asyncio\nimport asyncio\nfrom pyngrok import ngrok, conf\nimport subprocess # For running wget\nimport tempfile\nfrom moviepy.editor import VideoFileClip # For video processing\n\nfrom fastapi import FastAPI, File, UploadFile, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\n\n# --- Global ASR Model Setup ---\nprint(\"Loading ASR model globally...\")\ntry:\n    asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-0.6b-v2\")\n    asr_model.eval()\n    if torch.cuda.is_available():\n        print(\"Moving model to GPU and using half precision.\")\n        asr_model.cuda()\n        asr_model.half() \n    else:\n        print(\"CUDA not available, model will run on CPU.\")\n    print(\"ASR Model loaded successfully.\")\nexcept Exception as e:\n    print(f\"Error loading ASR model: {e}\")\n    asr_model = None # Set to None if loading fails\n\n# --- Example Audio Configuration ---\nEXAMPLE_AUDIO_FILENAME = \"example_audio_nemo.wav\" # Standard NeMo example\nEXAMPLE_AUDIO_DOWNLOAD_URL = \"https://dldata-public.s3.us-east-2.amazonaws.com/2086-149220-0033.wav\"\nKAGGLE_WORKING_DIR = Path(\"/kaggle/working/\")\nEXAMPLE_AUDIO_PATH = KAGGLE_WORKING_DIR / EXAMPLE_AUDIO_FILENAME\n\n# --- Directories for API ---\nTEMP_UPLOAD_DIR_API = KAGGLE_WORKING_DIR / \"api_temp_uploads\"\nTEMP_UPLOAD_DIR_API.mkdir(parents=True, exist_ok=True)\n\nPROCESSED_AUDIO_DIR_API = KAGGLE_WORKING_DIR / \"api_processed_audio\"\nPROCESSED_AUDIO_DIR_API.mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T00:04:58.595475Z","iopub.execute_input":"2025-05-15T00:04:58.596128Z","iopub.status.idle":"2025-05-15T00:05:48.843118Z","shell.execute_reply.started":"2025-05-15T00:04:58.596093Z","shell.execute_reply":"2025-05-15T00:05:48.842466Z"}},"outputs":[{"name":"stderr","text":"error: XDG_RUNTIME_DIR not set in the environment.\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","output_type":"stream"},{"name":"stdout","text":"Loading ASR model globally...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"parakeet-tdt-0.6b-v2.nemo:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0de51ecd0271408588eba88b3ececa56"}},"metadata":{}},{"name":"stdout","text":"[NeMo I 2025-05-15 00:05:38 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n","output_type":"stream"},{"name":"stderr","text":"[NeMo W 2025-05-15 00:05:38 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n    Train config : \n    use_lhotse: true\n    skip_missing_manifest_entries: true\n    input_cfg: null\n    tarred_audio_filepaths: null\n    manifest_filepath: null\n    sample_rate: 16000\n    shuffle: true\n    num_workers: 2\n    pin_memory: true\n    max_duration: 40.0\n    min_duration: 0.1\n    text_field: answer\n    batch_duration: null\n    use_bucketing: true\n    bucket_duration_bins: null\n    bucket_batch_size: null\n    num_buckets: 30\n    bucket_buffer_size: 20000\n    shuffle_buffer_size: 10000\n    \n[NeMo W 2025-05-15 00:05:38 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n    Validation config : \n    use_lhotse: true\n    manifest_filepath: null\n    sample_rate: 16000\n    batch_size: 16\n    shuffle: false\n    max_duration: 40.0\n    min_duration: 0.1\n    num_workers: 2\n    pin_memory: true\n    text_field: answer\n    \n","output_type":"stream"},{"name":"stdout","text":"[NeMo I 2025-05-15 00:05:38 nemo_logging:393] PADDING: 0\n[NeMo I 2025-05-15 00:05:45 nemo_logging:393] Using RNNT Loss : tdt\n    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n[NeMo I 2025-05-15 00:05:45 nemo_logging:393] Using RNNT Loss : tdt\n    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n[NeMo I 2025-05-15 00:05:45 nemo_logging:393] Using RNNT Loss : tdt\n    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n[NeMo I 2025-05-15 00:05:48 nemo_logging:393] Model EncDecRNNTBPEModel was successfully restored from /root/.cache/huggingface/hub/models--nvidia--parakeet-tdt-0.6b-v2/snapshots/50aec6a056e85b9f95b612df08a2bddc55b58714/parakeet-tdt-0.6b-v2.nemo.\nMoving model to GPU and using half precision.\nASR Model loaded successfully.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 3: Download Example Audio File\nif asr_model is not None: # Only proceed if model loaded\n    print(f\"Checking for example audio: {EXAMPLE_AUDIO_PATH}\")\n    if not EXAMPLE_AUDIO_PATH.exists() or EXAMPLE_AUDIO_PATH.stat().st_size == 0:\n        print(f\"Downloading example audio to {EXAMPLE_AUDIO_PATH}...\")\n        try:\n            subprocess.run([\"wget\", \"-O\", str(EXAMPLE_AUDIO_PATH), EXAMPLE_AUDIO_DOWNLOAD_URL], check=True, capture_output=True, text=True)\n            print(\"Download complete.\")\n            if EXAMPLE_AUDIO_PATH.exists() and EXAMPLE_AUDIO_PATH.stat().st_size > 0:\n                print(f\"Example audio file {EXAMPLE_AUDIO_PATH} is present and not empty.\")\n            else:\n                print(f\"Error: Example audio file {EXAMPLE_AUDIO_PATH} was not downloaded correctly or is empty after wget attempt.\")\n        except subprocess.CalledProcessError as e:\n            print(f\"Error downloading example audio with wget: {e}\")\n            print(f\"stderr: {e.stderr}\")\n        except FileNotFoundError:\n            print(\"Error: wget command not found. Please ensure wget is installed in the Kaggle environment.\")\n        except Exception as e_download:\n            print(f\"An unexpected error occurred during download: {e_download}\")\n    else:\n        print(f\"Example audio file {EXAMPLE_AUDIO_PATH} already exists.\")\nelse:\n    print(\"ASR model did not load, skipping example audio download.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T00:08:52.417819Z","iopub.execute_input":"2025-05-15T00:08:52.418446Z","iopub.status.idle":"2025-05-15T00:08:52.424441Z","shell.execute_reply.started":"2025-05-15T00:08:52.418423Z","shell.execute_reply":"2025-05-15T00:08:52.423549Z"}},"outputs":[{"name":"stdout","text":"Checking for example audio: /kaggle/working/example_audio_nemo.wav\nExample audio file /kaggle/working/example_audio_nemo.wav already exists.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Cell 4: Audio Preprocessing Function\ndef preprocess_audio_for_nemo(input_audio_path_str: str, output_processing_dir: Path) -> str:\n    \"\"\"\n    Preprocesses an audio file to be 16kHz mono WAV for NeMo.\n    Saves the processed file in output_processing_dir.\n    Returns the path (as a string) to the processed file.\n    \"\"\"\n    print(f\"Starting preprocessing for: {input_audio_path_str}\")\n    audio_path_obj = Path(input_audio_path_str)\n    \n    if not audio_path_obj.exists():\n        print(f\"Error: Input audio file not found at {input_audio_path_str}\")\n        raise FileNotFoundError(f\"Input audio file not found: {input_audio_path_str}\")\n\n    try:\n        audio = AudioSegment.from_file(input_audio_path_str)\n        print(f\"Original audio - Channels: {audio.channels}, Frame Rate: {audio.frame_rate}, Duration: {audio.duration_seconds:.2f}s\")\n    except Exception as e:\n        print(f\"Error loading audio file {input_audio_path_str} with pydub: {e}\")\n        raise\n    \n    resampled = False\n    mono = False\n    needs_export = False\n\n    target_sr = 16000\n    if audio.frame_rate != target_sr:\n        print(f\"Resampling audio from {audio.frame_rate}Hz to {target_sr}Hz\")\n        audio = audio.set_frame_rate(target_sr)\n        resampled = True\n        needs_export = True\n\n    if audio.channels != 1:\n        print(f\"Converting to mono (from {audio.channels} channels)\")\n        audio = audio.set_channels(1)\n        mono = True\n        needs_export = True\n\n    # Define the path for the processed file\n    # Use a consistent naming convention for the processed file in the specific output directory\n    processed_filename = f\"{audio_path_obj.stem}_processed_16kHz_mono.wav\"\n    processed_path_str = str(output_processing_dir / processed_filename)\n\n    if needs_export:\n        try:\n            audio.export(processed_path_str, format=\"wav\")\n            print(f\"Processed audio saved to: {processed_path_str}\")\n        except Exception as e:\n            print(f\"Error exporting processed audio to {processed_path_str}: {e}\")\n            raise\n        return processed_path_str\n    else:\n        # If no processing was strictly needed but we want to ensure it's in the target dir\n        # and has the standard processed name, copy it.\n        if str(audio_path_obj.resolve()) != str(Path(processed_path_str).resolve()):\n            print(f\"Audio was already 16kHz mono. Copying to target processing directory: {processed_path_str}\")\n            shutil.copy(input_audio_path_str, processed_path_str)\n            return processed_path_str\n        else:\n            print(\"Audio already 16kHz mono and in the correct location/name.\")\n            return input_audio_path_str\n\n# Function to extract audio from video\ndef extract_audio_from_video(video_path: str, output_dir: Path) -> str:\n    \"\"\"\n    Extracts audio from a video file and saves it as WAV.\n    Returns the path to the extracted audio file.\n    \"\"\"\n    video_path_obj = Path(video_path)\n    output_audio_path = str(output_dir / f\"{video_path_obj.stem}_audio.wav\")\n    \n    try:\n        print(f\"Extracting audio from video: {video_path}\")\n        video = VideoFileClip(video_path)\n        audio = video.audio\n        if audio is None:\n            raise ValueError(\"No audio stream found in the video file\")\n            \n        print(f\"Saving extracted audio to: {output_audio_path}\")\n        audio.write_audiofile(output_audio_path, fps=16000, nbytes=2, codec='pcm_s16le')\n        video.close()\n        \n        return output_audio_path\n    except Exception as e:\n        print(f\"Error extracting audio from video: {e}\")\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T00:08:56.461842Z","iopub.execute_input":"2025-05-15T00:08:56.462431Z","iopub.status.idle":"2025-05-15T00:08:56.472163Z","shell.execute_reply.started":"2025-05-15T00:08:56.462407Z","shell.execute_reply":"2025-05-15T00:08:56.471398Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Cell 5: FastAPI Application with Video Processing Endpoints\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\", \"http://localhost:3000\", \"https://localhost:3000\", \"http://localhost\", \"http://127.0.0.1:3000\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],  # Allows all methods\n    allow_headers=[\"*\"],  # Allows all headers\n    expose_headers=[\"*\"], # Expose all headers\n)\n\n# Keep original example endpoint for backward compatibility\n@app.post(\"/transcribe_example_audio/\")\nasync def transcribe_example_audio_endpoint(uploaded_file: UploadFile = File(None)): # uploaded_file is ignored\n    if asr_model is None:\n        print(\"Error: ASR model is not available (was not loaded in Cell 2).\")\n        raise HTTPException(status_code=503, detail=\"Transcription service is currently unavailable (model not loaded).\")\n\n    if not EXAMPLE_AUDIO_PATH.exists() or EXAMPLE_AUDIO_PATH.stat().st_size == 0:\n        print(f\"Critical Error: Example audio file {EXAMPLE_AUDIO_PATH} not found or empty on server. Cell 3 (download) might have failed.\")\n        raise HTTPException(status_code=500, detail=\"Server configuration error: Example audio file is missing.\")\n\n    processed_example_audio_path_str = None\n    try:\n        # Preprocess the *example* audio file (ensures it's in the right format for NeMo)\n        print(f\"Processing example audio: {EXAMPLE_AUDIO_PATH}\")\n        processed_example_audio_path_str = preprocess_audio_for_nemo(str(EXAMPLE_AUDIO_PATH), PROCESSED_AUDIO_DIR_API)\n        \n        # Ensure model is on the correct device and precision for transcription\n        if torch.cuda.is_available():\n            # Ensure model parameters are on CUDA device\n            if not next(asr_model.parameters()).is_cuda:\n                print(\"Model parameters were not on CUDA inside endpoint. Moving model to CUDA.\")\n                asr_model.cuda() # This moves the entire model, including parameters\n            \n            # Check current dtype of model parameters and apply .half() if not already float16\n            current_param_tensor = next(asr_model.parameters()).data # Get the underlying tensor data\n            if current_param_tensor.dtype != torch.float16:\n                if hasattr(asr_model, 'half') and callable(asr_model.half):\n                    print(f\"Model on CUDA but parameters dtype is {current_param_tensor.dtype}. Applying .half() for transcription.\")\n                    asr_model.half()\n                else:\n                    print(f\"Model on CUDA but parameters dtype is {current_param_tensor.dtype}, and .half() method is not available/callable.\")\n            else:\n                print(\"Model is already on CUDA and parameters are in float16 precision.\")\n        else:\n            # Optional: Ensure model is on CPU if CUDA is not available (defensive)\n            if next(asr_model.parameters()).is_cuda: \n                print(\"CUDA reported as not available, but model parameters are on CUDA. Moving to CPU.\")\n                asr_model.cpu() \n            print(\"Running transcription on CPU.\")\n        \n        print(f\"Transcribing processed example audio: {processed_example_audio_path_str}...\")\n        # Use timestamps=True, as confirmed by your original working Colab notebook\n        hypotheses = asr_model.transcribe([processed_example_audio_path_str], timestamps=True)\n\n        if not hypotheses or not isinstance(hypotheses, list) or not hypotheses[0]:\n            print(\"Transcription of example audio failed (no hypotheses returned from model.transcribe).\")\n            raise HTTPException(status_code=500, detail=\"Transcription of example audio failed (no hypotheses).\")\n\n        first_hypothesis = hypotheses[0]\n\n        # Based on your original Colab: output[0].timestamp['segment']\n        if not hasattr(first_hypothesis, 'timestamp') or 'segment' not in first_hypothesis.timestamp:\n            print(\"Example audio transcription output format error: 'timestamp' or 'segment' key missing in hypothesis.\")\n            # Log details for debugging\n            print(f\"Hypothesis object dir: {dir(first_hypothesis)}\")\n            if hasattr(first_hypothesis, 'text'): print(f\"Hypothesis text (if available): {first_hypothesis.text}\")\n            if hasattr(first_hypothesis, 'timestamp'): \n                ts_attr = first_hypothesis.timestamp\n                print(f\"Hypothesis timestamp attribute type: {type(ts_attr)}\")\n                if isinstance(ts_attr, dict):\n                    print(f\"Hypothesis timestamp keys: {ts_attr.keys()}\")\n                else:\n                    print(f\"Hypothesis timestamp attribute is not a dict.\")\n            else:\n                print(\"Hypothesis does not have a 'timestamp' attribute.\")\n            raise HTTPException(status_code=500, detail=\"Internal server error: Unexpected transcription output format for example audio.\")\n\n        segment_data = first_hypothesis.timestamp['segment']\n\n        if not segment_data: \n            print(\"No segments found in example audio transcription (audio might be silent or very short).\")\n            return JSONResponse(content={\"filename\": \"example_transcription.txt\", \"transcription_text\": \"\"})\n\n        transcript_lines = []\n        # Keys from your original Colab example: 'start', 'end', 'segment'\n        for s_data_dict in segment_data:\n            if not isinstance(s_data_dict, dict) or not all(k in s_data_dict for k in ['start', 'end', 'segment']):\n                print(f\"Segment data dictionary malformed or missing expected keys: {s_data_dict}\")\n                continue \n            start_time = s_data_dict['start']\n            end_time = s_data_dict['end']\n            text_segment = s_data_dict['segment']\n            transcript_lines.append(f\"{start_time:.2f}s - {end_time:.2f}s : {text_segment}\")\n        \n        final_transcript_text = \"\\n\".join(transcript_lines)\n        \n        print(\"Example audio transcription complete.\")\n        return JSONResponse(content={\"filename\": \"example_transcription.txt\", \"transcription_text\": final_transcript_text})\n\n    except FileNotFoundError as e:\n        print(f\"File not found during API processing (likely preprocess_audio_for_nemo failed to find input): {e}\")\n        raise HTTPException(status_code=500, detail=f\"Server error (file not found): {e}\")\n    except Exception as e:\n        print(f\"Error during example audio transcription endpoint: {e}\")\n        import traceback\n        traceback.print_exc() # This will print the full traceback to the notebook console\n        raise HTTPException(status_code=500, detail=f\"An error occurred during transcription: {str(e)}\")\n    finally:\n        # Clean up the processed example audio file (which was created by preprocess_audio_for_nemo)\n        if processed_example_audio_path_str and Path(processed_example_audio_path_str).exists():\n            try:\n                Path(processed_example_audio_path_str).unlink()\n                print(f\"Removed temporary processed example audio file: {processed_example_audio_path_str}\")\n            except OSError as e_unlink:\n                print(f\"Error removing temporary file {processed_example_audio_path_str}: {e_unlink}\")\n        # Note: The originally uploaded file (if any) is not explicitly handled here as it's ignored by this endpoint.\n        # The global ASR model is kept on its device (e.g., GPU) for subsequent requests.\n\n\n# NEW ENDPOINT: For processing actual video uploads\n@app.post(\"/transcribe_video/\")\nasync def transcribe_video_endpoint(uploaded_file: UploadFile = File(...)):  # The '...' makes it required\n    if asr_model is None:\n        print(\"Error: ASR model is not available (was not loaded in Cell 2).\")\n        raise HTTPException(status_code=503, detail=\"Transcription service is currently unavailable (model not loaded).\")\n\n    # Check for valid file\n    if not uploaded_file or not uploaded_file.filename:\n        raise HTTPException(status_code=400, detail=\"No file uploaded\")\n    \n    video_path = None\n    audio_path = None\n    processed_audio_path = None\n    \n    try:\n        # Create a temporary file to store the uploaded video\n        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(uploaded_file.filename).suffix) as tmp:\n            video_path = tmp.name\n            # Copy uploaded file to temp file\n            content = await uploaded_file.read()\n            tmp.write(content)\n        \n        print(f\"Video saved temporarily to: {video_path}\")\n        \n        # Extract audio from video\n        audio_path = extract_audio_from_video(video_path, TEMP_UPLOAD_DIR_API)\n        \n        # Preprocess the extracted audio\n        processed_audio_path = preprocess_audio_for_nemo(audio_path, PROCESSED_AUDIO_DIR_API)\n        \n        # Ensure model is on correct device and precision\n        if torch.cuda.is_available():\n            if not next(asr_model.parameters()).is_cuda:\n                asr_model.cuda()\n            \n            current_param_tensor = next(asr_model.parameters()).data\n            if current_param_tensor.dtype != torch.float16:\n                if hasattr(asr_model, 'half') and callable(asr_model.half):\n                    asr_model.half()\n        \n        # Transcribe the audio\n        print(f\"Transcribing video audio: {processed_audio_path}\")\n        hypotheses = asr_model.transcribe([processed_audio_path], timestamps=True)\n        \n        if not hypotheses or not isinstance(hypotheses, list) or not hypotheses[0]:\n            raise HTTPException(status_code=500, detail=\"Transcription failed (no results)\")\n        \n        first_hypothesis = hypotheses[0]\n        \n        if not hasattr(first_hypothesis, 'timestamp') or 'segment' not in first_hypothesis.timestamp:\n            print(f\"Unexpected transcription output format: {dir(first_hypothesis)}\")\n            raise HTTPException(status_code=500, detail=\"Unexpected transcription output format\")\n        \n        segment_data = first_hypothesis.timestamp['segment']\n        \n        if not segment_data:\n            return JSONResponse(content={\n                \"filename\": f\"{Path(uploaded_file.filename).stem}_transcription.txt\",\n                \"transcription_text\": \"\"\n            })\n        \n        transcript_lines = []\n        for s_data_dict in segment_data:\n            if not isinstance(s_data_dict, dict) or not all(k in s_data_dict for k in ['start', 'end', 'segment']):\n                continue\n            \n            start_time = s_data_dict['start']\n            end_time = s_data_dict['end']\n            text_segment = s_data_dict['segment']\n            transcript_lines.append(f\"{start_time:.2f}s - {end_time:.2f}s : {text_segment}\")\n        \n        final_transcript_text = \"\\n\".join(transcript_lines)\n        \n        print(\"Video transcription complete\")\n        return JSONResponse(content={\n            \"filename\": f\"{Path(uploaded_file.filename).stem}_transcription.txt\", \n            \"transcription_text\": final_transcript_text\n        })\n    \n    except Exception as e:\n        print(f\"Error during video transcription: {e}\")\n        import traceback\n        traceback.print_exc()\n        raise HTTPException(status_code=500, detail=f\"An error occurred during transcription: {str(e)}\")\n    \n    finally:\n        # Clean up temporary files\n        for path in [video_path, audio_path, processed_audio_path]:\n            if path and Path(path).exists():\n                try:\n                    Path(path).unlink()\n                    print(f\"Removed temporary file: {path}\")\n                except OSError as e:\n                    print(f\"Error removing temporary\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T00:18:23.574189Z","iopub.execute_input":"2025-05-15T00:18:23.574549Z","iopub.status.idle":"2025-05-15T00:18:23.601007Z","shell.execute_reply.started":"2025-05-15T00:18:23.574526Z","shell.execute_reply":"2025-05-15T00:18:23.600101Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Cell 6: Start Ngrok and Uvicorn Server\nNGROK_AUTH_TOKEN = \"2x43w3eV98ow7s4Esh2761K7Xib_5piK9Ef22bp5q45ZhT9E3\" # <--- REPLACE THIS WITH YOUR ACTUAL NGROK TOKEN\n\nif NGROK_AUTH_TOKEN == \"YOUR_NGROK_AUTHTOKEN\" or not NGROK_AUTH_TOKEN:\n    print(\"CRITICAL: Please set your NGROK_AUTH_TOKEN in Cell 6.\")\nelif asr_model is None:\n    print(\"CRITICAL: ASR model failed to load (see Cell 2). API will not start.\")\nelif not EXAMPLE_AUDIO_PATH.exists() or EXAMPLE_AUDIO_PATH.stat().st_size == 0:\n    print(f\"CRITICAL: Example audio file {EXAMPLE_AUDIO_PATH} is missing or empty (see Cell 3). API will not start.\")\nelse:\n    conf.get_default().auth_token = NGROK_AUTH_TOKEN\n    # Kill any existing ngrok tunnels\n    try:\n        tunnels = ngrok.get_tunnels()\n        for tunnel in tunnels:\n            ngrok.disconnect(tunnel.public_url)\n            print(f\"Disconnected previous tunnel: {tunnel.public_url}\")\n        ngrok.kill()\n        print(\"Killed all ngrok processes.\")\n    except Exception as e_ngrok_ops:\n        print(f\"Error managing ngrok processes (this is often okay if none were running): {e_ngrok_ops}\")\n\n    PORT = 7860 \n    try:\n        # Connect to ngrok\n        public_url_obj = ngrok.connect(PORT)\n        # Make sure we get a clean URL string\n        if hasattr(public_url_obj, 'public_url'):\n            public_url_str = public_url_obj.public_url\n        else:\n            public_url_str = str(public_url_obj).replace('NgrokTunnel: \"', '').replace('\" -> \"http://localhost:7860\"', '')\n        \n        # Ensure it's a clean URL string without extra quotes or NgrokTunnel text\n        if 'ngrok-free.app' in public_url_str and not public_url_str.startswith('http'):\n            public_url_str = f\"https://{public_url_str}\"\n\n        print(f\"Ngrok tunnel active: {public_url_str}\")\n        example_endpoint = f\"{public_url_str}/transcribe_example_audio/\"\n        video_endpoint = f\"{public_url_str}/transcribe_video/\"\n        \n        print(f\"Your example transcription API endpoint will be: {example_endpoint}\")\n        print(f\"Your video transcription API endpoint will be: {video_endpoint}\")\n        print(\"\\n*** IMPORTANT: UPDATE YOUR FRONTEND API.TS WITH THESE ENDPOINTS ***\")\n        print(\"In src/services/api.ts, update the fetch URL to:\")\n        print(f\"const response = await fetch('{video_endpoint}', {{\")\n        \n        # Run Uvicorn with a different approach for Jupyter notebooks\n        print(f\"\\nStarting Uvicorn server on port {PORT}...\")\n        import uvicorn.config\n        import asyncio\n        import threading\n        \n        server_config = uvicorn.config.Config(app=app, host=\"0.0.0.0\", port=PORT, log_level=\"info\")\n        server = uvicorn.server.Server(config=server_config)\n        \n        # Run the server in a separate thread to avoid asyncio issues\n        def run_server():\n            asyncio.set_event_loop(asyncio.new_event_loop())\n            server.run()\n        \n        thread = threading.Thread(target=run_server, daemon=True)\n        thread.start()\n        \n        # Verify the server is running\n        import time\n        import requests\n        print(\"Waiting for server to start...\")\n        time.sleep(2)  # Give the server a moment to start\n        \n        try:\n            health_check = requests.get(f\"http://localhost:{PORT}\")\n            print(f\"Server health check: {health_check.status_code}\")\n            print(f\"Server is running in background thread. Ngrok tunnel is active.\")\n            print(\"The API will continue running until you stop the notebook or kernel.\")\n            print(\"\\n*** After the server is running, test your endpoints with these commands: ***\")\n            print(f\"curl -X POST {example_endpoint}\")\n            print(f\"curl -X POST {video_endpoint} -F \\\"uploaded_file=@/path/to/your/video.mp4\\\"\")\n        except Exception as e:\n            print(f\"Server may not be fully started yet: {e}\")\n            print(\"Continue with the next steps anyway, the server should start soon.\")\n        \n    except Exception as e_ngrok_run:\n        print(f\"Could not start ngrok or Uvicorn server: {e_ngrok_run}\")\n        import traceback\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T00:19:38.272887Z","iopub.execute_input":"2025-05-15T00:19:38.273882Z","iopub.status.idle":"2025-05-15T00:19:40.924133Z","shell.execute_reply.started":"2025-05-15T00:19:38.273845Z","shell.execute_reply":"2025-05-15T00:19:40.923448Z"}},"outputs":[{"name":"stdout","text":"Killed all ngrok processes.\nNgrok tunnel active: https://3355-34-168-128-192.ngrok-free.app\nYour example transcription API endpoint will be: https://3355-34-168-128-192.ngrok-free.app/transcribe_example_audio/\nYour video transcription API endpoint will be: https://3355-34-168-128-192.ngrok-free.app/transcribe_video/\n\n*** IMPORTANT: UPDATE YOUR FRONTEND API.TS WITH THESE ENDPOINTS ***\nIn src/services/api.ts, update the fetch URL to:\nconst response = await fetch('https://3355-34-168-128-192.ngrok-free.app/transcribe_video/', {\n\nStarting Uvicorn server on port 7860...\n","output_type":"stream"},{"name":"stderr","text":"INFO:     Started server process [35]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 7860): address already in use\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\n","output_type":"stream"},{"name":"stdout","text":"Waiting for server to start...\nINFO:     127.0.0.1:49744 - \"GET / HTTP/1.1\" 404 Not Found\nServer health check: 404\nServer is running in background thread. Ngrok tunnel is active.\nThe API will continue running until you stop the notebook or kernel.\n\n*** After the server is running, test your endpoints with these commands: ***\ncurl -X POST https://3355-34-168-128-192.ngrok-free.app/transcribe_example_audio/\ncurl -X POST https://3355-34-168-128-192.ngrok-free.app/transcribe_video/ -F \"uploaded_file=@/path/to/your/video.mp4\"\nINFO:     2803:4600:1111:ee1:2dde:c401:5274:65ef:0 - \"POST /transcribe_video/ HTTP/1.1\" 422 Unprocessable Entity\nVideo saved temporarily to: /tmp/tmpwhnjda72.mp4\nExtracting audio from video: /tmp/tmpwhnjda72.mp4\nSaving extracted audio to: /kaggle/working/api_temp_uploads/tmpwhnjda72_audio.wav\nMoviePy - Writing audio in /kaggle/working/api_temp_uploads/tmpwhnjda72_audio.wav\n","output_type":"stream"},{"name":"stderr","text":"                                                                    ","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Starting preprocessing for: /kaggle/working/api_temp_uploads/tmpwhnjda72_audio.wav\nOriginal audio - Channels: 2, Frame Rate: 16000, Duration: 40.32s\nConverting to mono (from 2 channels)\nProcessed audio saved to: /kaggle/working/api_processed_audio/tmpwhnjda72_audio_processed_16kHz_mono.wav\nTranscribing video audio: /kaggle/working/api_processed_audio/tmpwhnjda72_audio_processed_16kHz_mono.wav\n[NeMo I 2025-05-15 00:25:14 nemo_logging:393] Timestamps requested, setting decoding timestamps to True. Capture them in Hypothesis object,                         with output[0][idx].timestep['word'/'segment'/'char']\n[NeMo I 2025-05-15 00:25:14 nemo_logging:393] Using RNNT Loss : tdt\n    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n","output_type":"stream"},{"name":"stderr","text":"[NeMo W 2025-05-15 00:25:14 nemo_logging:405] `include_duration` is not implemented for CUDA graphs\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]","output_type":"stream"},{"name":"stdout","text":"Video transcription complete\nRemoved temporary file: /tmp/tmpwhnjda72.mp4\nRemoved temporary file: /kaggle/working/api_temp_uploads/tmpwhnjda72_audio.wav\nRemoved temporary file: /kaggle/working/api_processed_audio/tmpwhnjda72_audio_processed_16kHz_mono.wav\nINFO:     2803:4600:1111:ee1:2dde:c401:5274:65ef:0 - \"POST /transcribe_video/ HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Video saved temporarily to: /tmp/tmpsc1n57_d.mp4\nExtracting audio from video: /tmp/tmpsc1n57_d.mp4\nSaving extracted audio to: /kaggle/working/api_temp_uploads/tmpsc1n57_d_audio.wav\nMoviePy - Writing audio in /kaggle/working/api_temp_uploads/tmpsc1n57_d_audio.wav\n","output_type":"stream"},{"name":"stderr","text":"                                                        ","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nStarting preprocessing for: /kaggle/working/api_temp_uploads/tmpsc1n57_d_audio.wav\nOriginal audio - Channels: 2, Frame Rate: 16000, Duration: 22.65s\nConverting to mono (from 2 channels)\nProcessed audio saved to: /kaggle/working/api_processed_audio/tmpsc1n57_d_audio_processed_16kHz_mono.wav\nTranscribing video audio: /kaggle/working/api_processed_audio/tmpsc1n57_d_audio_processed_16kHz_mono.wav\n[NeMo I 2025-05-15 00:30:02 nemo_logging:393] Timestamps requested, setting decoding timestamps to True. Capture them in Hypothesis object,                         with output[0][idx].timestep['word'/'segment'/'char']\n[NeMo I 2025-05-15 00:30:02 nemo_logging:393] Using RNNT Loss : tdt\n    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n","output_type":"stream"},{"name":"stderr","text":"[NeMo W 2025-05-15 00:30:02 nemo_logging:405] `include_duration` is not implemented for CUDA graphs\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.20it/s]","output_type":"stream"},{"name":"stdout","text":"Video transcription complete\nRemoved temporary file: /tmp/tmpsc1n57_d.mp4\nRemoved temporary file: /kaggle/working/api_temp_uploads/tmpsc1n57_d_audio.wav\nRemoved temporary file: /kaggle/working/api_processed_audio/tmpsc1n57_d_audio_processed_16kHz_mono.wav\nINFO:     2803:4600:1111:ee1:2dde:c401:5274:65ef:0 - \"POST /transcribe_video/ HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Video saved temporarily to: /tmp/tmpike6djbe.mp4\nExtracting audio from video: /tmp/tmpike6djbe.mp4\nSaving extracted audio to: /kaggle/working/api_temp_uploads/tmpike6djbe_audio.wav\nMoviePy - Writing audio in /kaggle/working/api_temp_uploads/tmpike6djbe_audio.wav\n","output_type":"stream"},{"name":"stderr","text":"                                                        ","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nStarting preprocessing for: /kaggle/working/api_temp_uploads/tmpike6djbe_audio.wav\nOriginal audio - Channels: 2, Frame Rate: 16000, Duration: 22.65s\nConverting to mono (from 2 channels)\nProcessed audio saved to: /kaggle/working/api_processed_audio/tmpike6djbe_audio_processed_16kHz_mono.wav\nTranscribing video audio: /kaggle/working/api_processed_audio/tmpike6djbe_audio_processed_16kHz_mono.wav\n[NeMo I 2025-05-15 00:32:22 nemo_logging:393] Timestamps requested, setting decoding timestamps to True. Capture them in Hypothesis object,                         with output[0][idx].timestep['word'/'segment'/'char']\n[NeMo I 2025-05-15 00:32:22 nemo_logging:393] Using RNNT Loss : tdt\n    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n","output_type":"stream"},{"name":"stderr","text":"[NeMo W 2025-05-15 00:32:22 nemo_logging:405] `include_duration` is not implemented for CUDA graphs\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.31it/s]","output_type":"stream"},{"name":"stdout","text":"Video transcription complete\nRemoved temporary file: /tmp/tmpike6djbe.mp4\nRemoved temporary file: /kaggle/working/api_temp_uploads/tmpike6djbe_audio.wav\nRemoved temporary file: /kaggle/working/api_processed_audio/tmpike6djbe_audio_processed_16kHz_mono.wav\nINFO:     2803:4600:1111:ee1:2dde:c401:5274:65ef:0 - \"POST /transcribe_video/ HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17}]}